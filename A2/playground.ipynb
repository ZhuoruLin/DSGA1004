{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing spark methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1005755d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.map(lambda x: 2*x).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums2 = sc.parallelize([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using union to combine RDDS!\n",
    "nums.union(nums2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pets = sc.parallelize(\\\n",
    "[(u\"cat\", 1), (u\"dog\", 1), (u\"cat\", 2)])\n",
    "pets_sorted = pets.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pets2 = sc.parallelize([(u\"simon\",1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inner join test\n",
    "rdd1 =  sc.parallelize([(\"foo\", 1), (\"bar\", 2), (\"baz\", 3)])\n",
    "rdd2 =  sc.parallelize([(\"foo\", None), (\"bar\", 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', (1, None)), ('bar', (2, 6))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import dataset\n",
    "from csv import reader\n",
    "parking_csv = sc.textFile(\"file:/Users/zhuorulin/Documents/DataScience/datasets/parking-violations.csv\")\n",
    "open_csv = sc.textFile('file:/Users/zhuorulin/Documents/DataScience/datasets/open-violations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "open_lines = open_csv.mapPartitions(lambda x: reader(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#map each entry in parking with value 1\n",
    "in_parking = parking_lines.map(lambda x: (x[0],1))\n",
    "in_open = open_lines.map(lambda x: (x[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = in_parking.union(in_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduced = combined.reduceByKey(lambda x,y:x+y).filter(lambda x:x[1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_parking_info(parking_line):\n",
    "    return (parking_line[0],parking_line[14]+' '+parking_line[6]+' '+parking_line[2]+parking_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying to take\n",
    "parking_info = parking_lines.map(extract_parking_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = parking_info.join(reduced).map(lambda x: (x[0],x[1][0])).sortByKey()\\\n",
    ".map(lambda x:'%s\\t%s'%(x[0],x[1])).saveAsTextFile('task1.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1017773099\\tGTR366A 23 242016-03-15',\n",
       " '1028883584\\tGJD3754 14 172016-03-10',\n",
       " '1028883651\\tJMM3606 103 202016-03-15',\n",
       " '1053108965\\tFMD6397 108 202016-03-28',\n",
       " '1076818079\\tDSB7845 108 142016-03-20',\n",
       " '1131602810\\tGWV8673 103 192016-03-25',\n",
       " '1131605718\\tFHV4978 109 202016-03-09',\n",
       " '1131606243\\tGPW5019 104 202016-03-08',\n",
       " '1131606267\\tGWX9386 103 242016-03-29',\n",
       " '1131607284\\tT655859C 103 142016-03-25']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./py_files/task1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./py_files/task1.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from csv import reader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #suppose: argv[1]:parking.csv argv[2]open.csv\n",
    "    sc = SparkContext()\n",
    "    parking_csv = sc.textFile(sys.argv[1],1)\n",
    "    open_csv = sc.textFile(sys.argv[2],1)\n",
    "    parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "    open_lines = open_csv.mapPartitions(lambda x: reader(x))\n",
    "    in_parking = parking_lines.map(lambda x: (x[0],1))\n",
    "    in_open = open_lines.map(lambda x: (x[0],-1))\n",
    "    combined = in_parking.union(in_open)\n",
    "    reduced = combined.reduceByKey(lambda x,y:x+y).filter(lambda x:x[1]==1)\n",
    "    def extract_parking_info(parking_line):\n",
    "        return (parking_line[0],parking_line[14]+', '+parking_line[6]+', '+parking_line[2]+', '+parking_line[1])\n",
    "    parking_info = parking_lines.map(extract_parking_info)\n",
    "    parking_info.join(reduced).map(lambda x: (x[0],x[1][0])).sortByKey().map(lambda x:'%s\\t%s'%(x[0],x[1])).saveAsTextFile(\"task1.out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\t159',\n",
       " '2\\t5',\n",
       " '3\\t92',\n",
       " '4\\t86',\n",
       " '5\\t10941',\n",
       " '6\\t23',\n",
       " '7\\t37584',\n",
       " '8\\t249',\n",
       " '9\\t733',\n",
       " '10\\t3930']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations_count = parking_lines.map(lambda x: (int(x[2]),1)).reduceByKey(lambda x,y:x+y)\\\n",
    ".sortBy(keyfunc = lambda x:x[0]).map(lambda x: '%s\\t%s'%(x[0],x[1]))\n",
    "violations_count.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./py_files/task2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./py_files/task2.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from csv import reader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #suppose: argv[1]:parking.csv argv[2]open.csv\n",
    "    sc = SparkContext()\n",
    "    parking_csv = sc.textFile(sys.argv[1],1)\n",
    "    #open_csv = sc.textFile(sys.argv[2],1)\n",
    "    parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "    #open_lines = open_csv.mapPartitions(lambda x: reader(x))\n",
    "    violations_count = parking_lines.map(lambda x: (int(x[2]),1)).reduceByKey(lambda x,y:x+y)\\\n",
    ".sortBy(keyfunc = lambda x:x[0]).map(lambda x: '%s\\t%s'%(x[0],x[1])).saveAsTextFile('task2.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open_due = open_lines.map(lambda x : (x[2],(float(x[12]),1))).sortByKey()\\\n",
    ".reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).sortByKey()\\\n",
    ".map(lambda x: '%s\\t%.2f, %.2f'%(x[0],x[1][0],x[1][0]/x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['999\\t246785.51, 85.51',\n",
       " 'AGC\\t255.00, 51.00',\n",
       " 'AGR\\t699.07, 58.26',\n",
       " 'AMB\\t0.00, 0.00',\n",
       " 'APP\\t96574.46, 63.12',\n",
       " 'ARG\\t0.00, 0.00',\n",
       " 'AYG\\t75.00, 18.75',\n",
       " 'BOB\\t0.00, 0.00',\n",
       " 'BOT\\t115.00, 115.00',\n",
       " 'CBS\\t5560.00, 86.88']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_due.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./py_files/task3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./py_files/task3.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from csv import reader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #suppose: argv[1]:parking.csv argv[2]open.csv\n",
    "    sc = SparkContext()\n",
    "    #parking_csv = sc.textFile(sys.argv[1],1)\n",
    "    open_csv = sc.textFile(sys.argv[1],1)\n",
    "    #parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "    open_lines = open_csv.mapPartitions(lambda x: reader(x))\n",
    "    open_lines.map(lambda x : (x[2],(float(x[12]),1))).sortByKey()\\\n",
    ".reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).sortByKey()\\\n",
    ".map(lambda x: '%s\\t%.2f, %.2f'%(x[0],x[1][0],x[1][0]/x[1][1])).saveAsTextFile('task3.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def task4map(line):\n",
    "    #If new york return ('NY',1)\n",
    "    #else return ('Other',1)\n",
    "    if line[16] =='NY':\n",
    "        return ('NY',1)\n",
    "    else:\n",
    "        return ('Other',1)\n",
    "parking_inNY_count = parking_lines.map(task4map).reduceByKey(lambda x,y:x+y).map(lambda x:'%s\\t%s'%(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NY\\t794106', 'Other\\t219911']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parking_inNY_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./py_files/task4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./py_files/task4.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from csv import reader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #suppose: argv[1]:parking.csv argv[2]open.csv\n",
    "    sc = SparkContext()\n",
    "    parking_csv = sc.textFile(sys.argv[1],1)\n",
    "    #open_csv = sc.textFile(sys.argv[2],1)\n",
    "    parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "    #open_lines = open_csv.mapPartitions(lambda x: reader(x))\n",
    "    def task4map(line):\n",
    "    #If new york return ('NY',1)\n",
    "    #else return ('Other',1)\n",
    "        if line[16] =='NY':\n",
    "            return ('NY',1)\n",
    "        else:\n",
    "            return ('Other',1)\n",
    "    parking_lines.map(task4map).reduceByKey(lambda x,y:x+y).map(lambda x:'%s\\t%s'%(x[0],x[1]))\\\n",
    "    .saveAsTextFile('task4.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parking_vehicle_count = parking_lines.map(lambda x:('%s, %s'%(x[14],x[16]),1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evil = sc.parallelize(parking_vehicle_count.take(1)).map(lambda x:'%s\\t%s'%(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLANKPLATE, 99/t1203']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evil.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./py_files/task5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./py_files/task5.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from csv import reader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #suppose: argv[1]:parking.csv argv[2]open.csv\n",
    "    sc = SparkContext()\n",
    "    parking_csv = sc.textFile(sys.argv[1],1)\n",
    "    #open_csv = sc.textFile(sys.argv[2],1)\n",
    "    parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "    #open_lines = open_csv.mapPartitions(lambda x: reader(x))\n",
    "    parking_vehicle_count = parking_lines.map(lambda x:('%s, %s'%(x[14],x[16]),1))\\\n",
    "    .reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],False)\n",
    "    sc.parallelize(parking_vehicle_count.take(1)).map(lambda x:'%s\\t%s'%(x[0],x[1]))\\\n",
    "    .saveAsTextFile('task5.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLANKPLATE, 99/t1203',\n",
       " 'N/A, NY/t155',\n",
       " 'AP501F, NJ/t138',\n",
       " '56207MG, NY/t125',\n",
       " '20302TC, NY/t116',\n",
       " '96087MA, NY/t116',\n",
       " 'AR290A, NJ/t114',\n",
       " '12359MG, NY/t110',\n",
       " '17741MD, NY/t107',\n",
       " '12817KA, NY/t107',\n",
       " '96091MA, NY/t103',\n",
       " '96089MA, NY/t102',\n",
       " 'AP300F, NJ/t101',\n",
       " '62546JM, NY/t100',\n",
       " '81091MB, NY/t100',\n",
       " '14483JY, NY/t99',\n",
       " 'AL353U, NJ/t99',\n",
       " '16206TC, NY/t97',\n",
       " '55109MB, NY/t97',\n",
       " '30954JX, NY/t96']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parking_vehicle_count = parking_lines.map(lambda x:('%s, %s'%(x[14],x[16]),1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],False)\n",
    "evil = sc.parallelize(parking_vehicle_count.take(20)).map(lambda x:'%s\\t%s'%(x[0],x[1]))\n",
    "evil.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./py_files/task6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./py_files/task6.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from csv import reader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #suppose: argv[1]:parking.csv argv[2]open.csv\n",
    "    sc = SparkContext()\n",
    "    parking_csv = sc.textFile(sys.argv[1],1)\n",
    "    #open_csv = sc.textFile(sys.argv[2],1)\n",
    "    parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "    #open_lines = open_csv.mapPartitions(lambda x: reader(x))\n",
    "    parking_vehicle_count = parking_lines.map(lambda x:('%s, %s'%(x[14],x[16]),1))\\\n",
    "    .reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],False)\n",
    "    sc.parallelize(parking_vehicle_count.take(20)).map(lambda x:'%s\\t%s'%(x[0],x[1]))\\\n",
    "    .saveAsTextFile('task6.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def task7map(line):\n",
    "    issue_date = line[1]\n",
    "    issue_day = int(issue_date.split('-')[2])\n",
    "    violation_code = line[2]\n",
    "    if issue_day in [5,6,12,13,19,20,26,27]:\n",
    "        return ((int(violation_code),(1,0)))\n",
    "    else:\n",
    "        return ((int(violation_code),(0,1)))\n",
    "code_isWeekend_count = parking_lines.map(task7map).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).sortByKey()\\\n",
    ".map(lambda x: '%s\\t%.2f,%.2f'%(x[0],x[1][0]/8.,x[1][1]/23.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\t3.25,5.78',\n",
       " '2\\t0.12,0.17',\n",
       " '3\\t1.12,3.61',\n",
       " '4\\t6.25,1.57',\n",
       " '5\\t0.00,475.70',\n",
       " '6\\t0.88,0.70',\n",
       " '7\\t1359.75,1161.13',\n",
       " '8\\t4.38,9.30',\n",
       " '9\\t2.00,31.17',\n",
       " '10\\t43.62,155.70']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_isWeekend_count.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task7.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./py_files/task7.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from csv import reader\n",
    "\n",
    "def task7map(line):\n",
    "    issue_date = line[1]\n",
    "    issue_day = int(issue_date.split('-')[2])\n",
    "    violation_code = line[2]\n",
    "    if issue_day in [5,6,12,13,19,20,26,27]:\n",
    "        return ((int(violation_code),(1,0)))\n",
    "    else:\n",
    "        return ((int(violation_code),(0,1)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #suppose: argv[1]:parking.csv argv[2]open.csv\n",
    "    sc = SparkContext()\n",
    "    parking_csv = sc.textFile(sys.argv[1],1)\n",
    "    #open_csv = sc.textFile(sys.argv[2],1)\n",
    "    parking_lines = parking_csv.mapPartitions(lambda x: reader(x))\n",
    "    #open_lines = open_csv.mapPartitions(lambda x: reader(x))\n",
    "    parking_lines.map(task7map).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).sortByKey()\\\n",
    ".map(lambda x: '%s\\t%.2f,%.2f'%(x[0],x[1][0]/8.,x[1][1]/23.)).saveAsTextFile('task7.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
